{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this [experiment](Question-Answering.ipynb), I fine-tuned a BERT model on the SQuAD dataset for the task of extractive question answering. (I learned the general data preprocessing, training and evaluation approach from this [tutorial](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt) on Hugging Face NLP course, and wrote my own scripts.)\n",
    "\n",
    "The goal is to identify the correct span of text in a document (context) that answers a given question. During data preparation, I converted the ground-truth answers to labels used for training. Specifically, these labels were the start and end token positions within the \"tokenized context.\" \n",
    "\n",
    "On the SQuAD metric, BERT had an F1 score on validation set of 4% before finetuning and 80% after finetuning. The **Results** section below show some context paragraphs, questions and extracted answers. After finetuning, BERT seems able to spot the answers even after I tried to phrase the questions in highly abstract terms and avoid the more obvious wording from the original context.\n",
    "\n",
    "The experiment demonstrated BERT's capability to perform on downstream NLP tasks - in this case accurately extracting factual answers from context. It also helped me practice some techniques such as\n",
    "- handling long contexts by creating multiple training examples (using a sliding window), ensuring all relevant parts of the context are captured for the model\n",
    "- using the tokenizer-generated offset mappings to correctly align the token indices with the original answer spans\n",
    "- evaluating the model's predictions using the SQuAD benchmark, which came off-the-shelf from the `evaluate` library\n",
    "\n",
    "## Verifying the evaluation code\n",
    "\n",
    "The model basically scores start and end tokens within the context to assess how likely each token should be the start and end of the answer. During inference, to make predictions, we try to select a pair of start and end tokens that would together have the highest probability, meaning they represent the most correct answer. (Some pairs are excluded from consideration - please see the code or the original tutorial for further details.)\n",
    "\n",
    "I implemented a vectorized version (relying on matrix operations) of this inference process, instead of using the steps outlined in the tutorial. To verify my implementation of evaluation code, I ran a DistilBERT model that was already fine-tuned on SQuAD on a small validation set. As expected, the SQuAD-pretrained BERT performed well with the `evaluate.load('squad')` benchmark.\n",
    "\n",
    "```\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)\n",
    "```\n",
    "\n",
    "Results:\n",
    "```\n",
    "{'exact_match': 75.25, 'f1': 78.58002727406753}\n",
    "```\n",
    "\n",
    "Meanwhile, the pre-finetuned BERT model scored poorly before finetuning:\n",
    "```\n",
    "{'exact_match': 0.0, 'f1': 4.363390189980071}\n",
    "```\n",
    "\n",
    "## Training\n",
    "\n",
    "The model is then trained and evaluated using the same SQuAD benchmark. It scored much better on the validation data after fine-tuning.\n",
    "\n",
    "```\n",
    "{'exact_match': 71.1, 'f1': 80.62985630352878}\n",
    "```\n",
    "\n",
    "![Training loss curve](training-loss-curve.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# so that the Tensor representation in debugging view shows the tensor shape\n",
    "normal_repr = torch.Tensor.__repr__ \n",
    "torch.Tensor.__repr__ = lambda self: f\"{self.shape}_{normal_repr(self)}\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google-bert/bert-base-cased\"\n",
    "dataset_name = \"squad\"\n",
    "\n",
    "# for tokenizers\n",
    "max_length = 384\n",
    "stride = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b82cb7f5a9495ba88121806a8e4e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f248ee642a4b46b54f29226c5404b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40027c8b06da448d8c1641024c664e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b749b9eb921642debf992c3aec9fa480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/HF_NLP/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the 'squad' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b069dae1270446f9107b65264e20892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febfc1c0fbf0455498cd60c6b92ee2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78eb47c97e5140dcbf6059acf86ed647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec397c02b1248979f8abd65b4687516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3265a8ad73ba4088ab6698122f1de8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "dataset_squad_original: Dataset = load_dataset(\"squad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_squad_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a smaller version\n",
    "dataset_squad_mini: Dataset = DatasetDict({\n",
    "    \"train\": dataset_squad_original[\"train\"].select(range(50000)),\n",
    "    \"valid\": dataset_squad_original[\"validation\"].select(range(5000)),}\n",
    ")\n",
    "# info(dataset_squad_original)\n",
    "dataset_squad_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the dataset \"train\" split\n",
    "dataset_squad_train: Dataset = dataset_squad_mini[\"train\"]\n",
    "dataset_squad_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and creating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_create_label(batch):\n",
    "    \n",
    "    questions = [q.strip() for q in batch[\"question\"]]\n",
    "    # questions = batch[\"question\"]\n",
    "    batch_tokenized = tokenizer(text=questions, text_pair=batch[\"context\"],\n",
    "                                                   truncation=\"only_second\", max_length=max_length, \n",
    "                                                   padding=\"max_length\",\n",
    "                                                   stride=stride,\n",
    "                                                   return_overflowing_tokens=True, \n",
    "                                                #    return_length=True,\n",
    "                                                   return_offsets_mapping=True,\n",
    "                                                   )\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    # loop through each \"tokenized example\"\n",
    "    # for each \"tokenized example\",\n",
    "    for tokenized_example_idx in range(len(batch_tokenized[\"input_ids\"])):\n",
    "        # find out the corresponding id of the corresponding \"original example\"\n",
    "        id_original_example = batch_tokenized[\"overflow_to_sample_mapping\"][tokenized_example_idx]\n",
    "        \n",
    "        # DEBUGGING:\n",
    "        if batch[\"id\"][id_original_example] in [\"570ba1b2ec8fbc190045ba8e\"]:\n",
    "            pass\n",
    "        \n",
    "        # determine the start and end indices of the answer within the \"original example\"\n",
    "        answer_text = batch[\"answers\"][id_original_example][\"text\"][0]\n",
    "        answer_start_character_position = batch[\"answers\"][id_original_example][\"answer_start\"][0]\n",
    "        answer_end_character_position = answer_start_character_position + len(answer_text) - 1\n",
    "    \n",
    "        # for this \"tokenized example,\" see if those start and end indices occur within this tokenized example\n",
    "        # if not, then the label (start position, end position) for this particular tokenized example should be 0, 0\n",
    "        # if found, then the label (start position, end position) for this particular tokenized example should be the indices of the tokens within this \"tokenized example\"\n",
    "        # first, find out the position of first token within this tokenized example that is the start of the context\n",
    "        # from that position, increment repeatedly to find out the position of the token that corresponds to the start of the answer\n",
    "        # then, from the last position within this tokenized example, decrement repeatedly to find out the position that corresponds to the end of the answer\n",
    "        # build up the start_positions and end_positions list\n",
    "        token_type_ids = batch_tokenized.sequence_ids(tokenized_example_idx)\n",
    "        for i in range(len(token_type_ids)):\n",
    "            if (token_type_ids[i] == 1):\n",
    "                break\n",
    "        token_idx_start_of_context = i\n",
    "        for i in range(len(token_type_ids) - 1, token_idx_start_of_context - 1, -1):\n",
    "            if token_type_ids[i] == 1:\n",
    "                break\n",
    "        token_idx_end_of_context = i\n",
    "    \n",
    "        start_of_context_mapping_tuple = batch_tokenized[\"offset_mapping\"][tokenized_example_idx][token_idx_start_of_context]\n",
    "        end_of_context_mapping_tuple = batch_tokenized[\"offset_mapping\"][tokenized_example_idx][token_idx_end_of_context]\n",
    "        if start_of_context_mapping_tuple[0] > answer_start_character_position or end_of_context_mapping_tuple[1] < (answer_end_character_position + 1):\n",
    "            label = (0, 0)\n",
    "        else:\n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            for i in range(token_idx_start_of_context, len(batch_tokenized[\"offset_mapping\"][tokenized_example_idx])):\n",
    "                offset_mapping_tuple = batch_tokenized[\"offset_mapping\"][tokenized_example_idx][i]\n",
    "                if offset_mapping_tuple[0] > answer_start_character_position:\n",
    "                    start_position = i - 1\n",
    "                    break\n",
    "            if start_position is None:\n",
    "                start_position = token_idx_end_of_context\n",
    "                \n",
    "            for i in range(token_idx_end_of_context, token_idx_start_of_context - 1, -1):\n",
    "                offset_mapping_tuple = batch_tokenized[\"offset_mapping\"][tokenized_example_idx][i]\n",
    "                if offset_mapping_tuple[1] < (answer_end_character_position + 1):\n",
    "                    end_position = i + 1\n",
    "                    break\n",
    "            if end_position is None:\n",
    "                end_position = token_idx_start_of_context\n",
    "            \n",
    "            label = (start_position, end_position)\n",
    "\n",
    "            # double check\n",
    "            double_check_start_char_in_context = batch_tokenized[\"offset_mapping\"][tokenized_example_idx][start_position][0]\n",
    "            double_check_end_char_in_context = batch_tokenized[\"offset_mapping\"][tokenized_example_idx][end_position][1]\n",
    "            double_check_ans_text = batch[\"context\"][id_original_example][double_check_start_char_in_context : double_check_end_char_in_context]\n",
    "            if double_check_ans_text != answer_text:\n",
    "                # print(double_check_ans_text)\n",
    "                # print(answer_text)\n",
    "                # pdb.set_trace()\n",
    "                label = (0, 0)\n",
    "                print(f\"WARNIN': Mismatched answer spans. Ignored. Id: {batch[\"id\"][id_original_example]}. Ground-truth: {answer_text}. Extracted: {double_check_ans_text}\")\n",
    "                # assert double_check_ans_text == answer_text\n",
    "                \n",
    "            \n",
    "            \n",
    "        start_positions.append(label[0])\n",
    "        end_positions.append(label[1])\n",
    "        # print(start_positions, \" * \", end_positions)\n",
    "    batch_tokenized[\"start_positions\"] = start_positions\n",
    "    batch_tokenized[\"end_positions\"] = end_positions\n",
    "    \n",
    "    if \"token_type_ids\" not in batch_tokenized.keys():\n",
    "        token_type_ids = []\n",
    "        for tokenized_example_idx in range(len(batch_tokenized[\"input_ids\"])):\n",
    "            token_type_sequence = [1 if token_type == 1 else 0\n",
    "                                   for token_type in batch_tokenized.sequence_ids((tokenized_example_idx))]\n",
    "            token_type_ids.append(token_type_sequence)\n",
    "            \n",
    "        batch_tokenized[\"token_type_ids\"] = token_type_ids  \n",
    "    \n",
    "    # print(len(batch_tokenized[\"input_ids\"]), len(start_positions))\n",
    "    return batch_tokenized\n",
    "\n",
    "# tokenize_and_create_labels(dataset_squad_train_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acd398983254a1aaf33fceebc27ba7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNIN': Mismatched answer spans. Ignored. Id: 56d62f3e1c85041400946fa5. Ground-truth: hunt. Extracted: hunting\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e10ac2cd28a01900c674bb. Ground-truth: 5. Extracted: 35\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf36e5aab44d1400b88e71. Ground-truth: 65,000. Extracted: 865,000\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56be973d3aeaaa14008c9123. Ground-truth: six. Extracted: sixth\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56de0abc4396321400ee2563. Ground-truth: Islam. Extracted: Islamic\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df86855ca0a614008f9c1d. Ground-truth: 3. Extracted: 35\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572628deec44d21400f3daaf. Ground-truth: refor. Extracted: reforms\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57267bf95951b619008f7447. Ground-truth: 32. Extracted: 32nd\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56dfa6de7aa994140058df9b. Ground-truth: no. Extracted: notes\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf8915234ae51400d9be15. Ground-truth: 3. Extracted: 35\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cdcf7d62d2951400fa686d. Ground-truth: M. Extracted: Mallory\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e6eb396fe0821900b8ec27. Ground-truth: b. Extracted: been\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf4113aab44d1400b88ee0. Ground-truth: four. Extracted: fourth\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cd682162d2951400fa658e. Ground-truth: Tibet. Extracted: Tibetan\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57280b532ca10214002d9c7e. Ground-truth: Some people with asthma rarely experience symptoms, u. Extracted: Some people with asthma rarely experience symptoms, usually\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df5e8e96943c1400a5d44d. Ground-truth: 2. Extracted: 27\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf64d24df3c31400b0d6f5. Ground-truth: 3. Extracted: 2003\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 573189d6e6313a140071d066. Ground-truth: y science-fiction and adventure. Extracted: early science-fiction and adventure\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57265693dd62a815002e81ed. Ground-truth: often scripted to dispense with the appearance of neutrality and use their influence to unfairly influence the outcome of the match for added dramatic impact. F. Extracted: often scripted to dispense with the appearance of neutrality and use their influence to unfairly influence the outcome of the match for added dramatic impact. Face\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e722d937bdd419002c3d8f. Ground-truth: extreme. Extracted: extremely\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df623596943c1400a5d494. Ground-truth: 20. Extracted: 2009\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56ce726faab44d1400b88793. Ground-truth: 3. Extracted: 23\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf657b4df3c31400b0d6ff. Ground-truth: 2. Extracted: 2004\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df98d938dc4217001520b6. Ground-truth: 5. Extracted: 58\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e1500fcd28a01900c677a1. Ground-truth: four. Extracted: fourth\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56d3ac8e2ccc5a1400d82e1b. Ground-truth: opera. Extracted: operatic\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56bf7e603aeaaa14008c9681. Ground-truth: split with Luckett and Rober. Extracted: split with Luckett and Roberson\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56dfb5977aa994140058e02e. Ground-truth: 3. Extracted: 23\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56f88857a6d7ea1400e1772e. Ground-truth: continental Europe. Extracted: continental European\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df844f56340a1900b29cca. Ground-truth: 60 w. Extracted: 60 wa\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56f7083a711bf01900a448f6. Ground-truth: peaceful. Extracted: peacefully\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56de57394396321400ee2830. Ground-truth: six. Extracted: sixth\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cd8ffa62d2951400fa6723. Ground-truth: Japan. Extracted: Japanese\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df95d44a1a83140091eb81. Ground-truth: ano. Extracted: another\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56dd1cf99a695914005b94d8. Ground-truth: southwest. Extracted: southwestern\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56d0f8a117492d1400aab6af. Ground-truth: 3. Extracted: 32\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57265e2b5951b619008f70b6. Ground-truth: orida had become \"a derelict open to the occupancy of every enemy, civilized or savage. Extracted: Florida had become \"a derelict open to the occupancy of every enemy, civilized or savage\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56f7188d711bf01900a44954. Ground-truth: region. Extracted: regional\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df86c056340a1900b29cf6. Ground-truth: negative. Extracted: negatively\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56d366ee59d6e414001462f6. Ground-truth: 15. Extracted: 2015\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56ce750daab44d1400b887b4. Ground-truth: 5. Extracted: 2015\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572ebe0a03f98919007569d2. Ground-truth: \"While the interstellar absorbing medium may be simply the ether, [it] is characteris. Extracted: \"While the interstellar absorbing medium may be simply the ether, [it] is characteristic\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56de4796cffd8e1900b4b777. Ground-truth: symbiosis. Extracted: osymbiosis\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572a50757a1753140016ae9e. Ground-truth: s when too much water is drawn into the bowels. Extracted: occurs when too much water is drawn into the bowels\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56d13983e7d4791d00902041. Ground-truth: 3. Extracted: 35\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf2e15aab44d1400b88dcb. Ground-truth: four. Extracted: fourth\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572670f2f1498d1400e8dfb6. Ground-truth: 966 Claude R. Kirk, Jr. was elected as the first post-Reconstruction Republican governor, in an upset election. Extracted: 1966 Claude R. Kirk, Jr. was elected as the first post-Reconstruction Republican governor, in an upset election\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df865956340a1900b29ceb. Ground-truth: kno. Extracted: known\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56d1b926e7d4791d009020cf. Ground-truth: 4. Extracted: 4th\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5728384dff5b5019007d9f4e. Ground-truth: ederalism in the United States is the evolving relationship between state governments and the federal government of the United States. Extracted: Federalism in the United States is the evolving relationship between state governments and the federal government of the United States\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df5cdd96943c1400a5d438. Ground-truth: 5. Extracted: 500\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e1184ecd28a01900c6759a. Ground-truth: high. Extracted: higher\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572783c7f1498d1400e8fa60. Ground-truth: e semi-finals have been played exclusively at the rebuilt Wembley Stadium. Extracted: The semi-finals have been played exclusively at the rebuilt Wembley Stadium\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e6eb396fe0821900b8ec28. Ground-truth: v. Extracted: River\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57309ede396df9190009621b. Ground-truth: rld around it. Extracted: world around it\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 570ffff5b654c5140001f725. Ground-truth: x. Men did not show any sexual arousal to non-human visual stimuli,. Extracted: sex. Men did not show any sexual arousal to non-human visual stimuli,\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56d383b159d6e414001465e6. Ground-truth: 15. Extracted: 2015\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57262473271a42140099d4ea. Ground-truth: Buddh. Extracted: Buddhism\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf4e9aaab44d1400b88f9c. Ground-truth: 7. Extracted: 2007\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572628f0271a42140099d623. Ground-truth: tes (3. Extracted: tes (304\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572670f2f1498d1400e8dfb7. Ground-truth: 968 Edward J. Gurney, also a white conservative, was elected as the state's first post-reconstruction Republican US Senator. Extracted: 1968 Edward J. Gurney, also a white conservative, was elected as the state's first post-reconstruction Republican US Senator\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 570d2681fed7b91900d45c65. Ground-truth: Europe. Extracted: European\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df7f755ca0a614008f9b62. Ground-truth: north. Extracted: northern\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57262957ec44d21400f3daff. Ground-truth: y of. Extracted: many of\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e4744d39bdeb140034793b. Ground-truth: Architect. Extracted: Architecture\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56f7d4f7aef2371900625c25. Ground-truth: no. Extracted: nobility\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cebbdeaab44d1400b8895c. Ground-truth: 1 million. Extracted: 11 million\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e0bc7b231d4119001ac364. Ground-truth: December 6, 195. Extracted: December 6, 1957\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5725bc3989a1e219009abda6. Ground-truth: n March 2006, the foundation announced a US$5 million grant for the International Justice Mission (IJM). Extracted: In March 2006, the foundation announced a US$5 million grant for the International Justice Mission (IJM)\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cffb63234ae51400d9c1e7. Ground-truth: 2. Extracted: 200\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e8638c37bdd419002c44e1. Ground-truth: 8. Extracted: 148\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e4793839bdeb140034794f. Ground-truth: environment. Extracted: environmental\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5706300775f01819005e7a62. Ground-truth: dard. Extracted: standard\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5732a0c6cc179a14009dab9e. Ground-truth: 28 July 180. Extracted: 28 July 1800\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cbdea66d243a140015edae. Ground-truth: 7. Extracted: 1817\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56ccde7862d2951400fa64d9. Ground-truth: Tibet. Extracted: Tibetan\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e1b959cd28a01900c67ad1. Ground-truth: Somali Post. Extracted: Somali Postal\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e08070231d4119001ac203. Ground-truth: 2. Extracted: 290\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df9ad738dc4217001520c8. Ground-truth: 9. Extracted: 1897\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e6eb396fe0821900b8ec26. Ground-truth: n. Extracted: Nanjing\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57266ac25951b619008f7216. Ground-truth: evolution. Extracted: evolutionary\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56d38b4e59d6e414001466d7. Ground-truth: 9. Extracted: 2009\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e142e6e3433e1400422d10. Ground-truth: 22. Extracted: 227\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572908166aef0514001549cb. Ground-truth: privately funded English language schoo. Extracted: privately funded English language schools\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e080dc231d4119001ac20d. Ground-truth: 1. Extracted: 10\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56deebdc3277331400b4d81f. Ground-truth: one. Extracted: component\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e7632700c9c71400d7707f. Ground-truth: 2. Extracted: 216\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56dfbc1d231d4119001abd48. Ground-truth: light. Extracted: lighting\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56df9dbd4a1a83140091eb99. Ground-truth: 3. Extracted: 1834\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cd5d3a62d2951400fa653e. Ground-truth: manual. Extracted: manually\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 571047cea58dae1900cd6999. Ground-truth: avoid. Extracted: avoiding\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e0840c231d4119001ac23a. Ground-truth: 3. Extracted: 230\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e079df231d4119001ac19f. Ground-truth: 1. Extracted: 2010\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf657b4df3c31400b0d700. Ground-truth: 2. Extracted: 2004\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5728b4714b864d1900164c70. Ground-truth: Joseph Bohm. Extracted: Joseph Bohmann\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572629d9ec44d21400f3db29. Ground-truth: The company's mainstay business. Extracted: The company's mainstay businesses\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e42c8439bdeb1400347913. Ground-truth: one. Extracted: phone\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e7af3b37bdd419002c4338. Ground-truth: 16. Extracted: 2016\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e08a18231d4119001ac291. Ground-truth: negative. Extracted: electronegative\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf829c234ae51400d9bdd4. Ground-truth: 2. Extracted: 2002\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e06db77aa994140058e49f. Ground-truth: 15. Extracted: 158\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cde1f462d2951400fa6961. Ground-truth: 2. Extracted: 280\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5731d3b3e17f3d1400422461. Ground-truth: Indirect. Extracted: Indirectly\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5727c1064b864d1900163c87. Ground-truth: e wings of flightless birds and the rudiments of pelvis and leg bones found in some snakes. Extracted: the wings of flightless birds and the rudiments of pelvis and leg bones found in some snakes\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572f3442b2c2fd1400567f83. Ground-truth: 40 million years ago. Extracted: 140 million years ago\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56dfa0414a1a83140091ebad. Ground-truth: 4. Extracted: 48\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e042487aa994140058e409. Ground-truth: p. Extracted: Alpha\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5728f8422ca10214002dab4c. Ground-truth: 5 international airlines and increasing numbers of airlines have began launching direct flights from Japan, Qatar, Taiwan, South Korea, Germany and Singapore.. Extracted: 15 international airlines and increasing numbers of airlines have began launching direct flights from Japan, Qatar, Taiwan, South Korea, Germany and Singapore.\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56dafad1e7c41114004b4bfc. Ground-truth: 2. Extracted: 72\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5726298fec44d21400f3db09. Ground-truth: in. Extracted: invasions\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 572bfa9ef182dd1900d7c7a3. Ground-truth: German. Extracted: Germanic\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e6eb396fe0821900b8ec24. Ground-truth: ks. Extracted: networks\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e796f800c9c71400d7731f. Ground-truth: 16. Extracted: 2016\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57294d87af94a219006aa278. Ground-truth: Prussia. Extracted: Prussian\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e17841cd28a01900c67997. Ground-truth: cycling. Extracted: icycling\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 5726d6b55951b619008f7f8e. Ground-truth: 2. Extracted: 23\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 570de5f70b85d914000d7b9c. Ground-truth: large. Extracted: largest\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e07d387aa994140058e56d. Ground-truth: 1. Extracted: 1967\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cfe987234ae51400d9c09b. Ground-truth: 92. Extracted: 1892\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 571a275210f8ca1400304f06. Ground-truth: g allows information from the outside world to be sensed in the form of chemical and physical stimuli.. Extracted: ding allows information from the outside world to be sensed in the form of chemical and physical stimuli.\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56e098707aa994140058e605. Ground-truth: 91. Extracted: 1917\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56dfa04b38dc421700152128. Ground-truth: men. Extracted: arrangements\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 570dfa320b85d914000d7c48. Ground-truth: No. Extracted: Nonetheless\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57318409e6313a140071cffa. Ground-truth: Inca. Extracted: Incans\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 56cf3442aab44d1400b88e39. Ground-truth: 3. Extracted: 33\n",
      "WARNIN': Mismatched answer spans. Ignored. Id: 57262473271a42140099d4eb. Ground-truth: m and E. Extracted: m and Epic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'],\n",
       "    num_rows: 50662\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_squad_train_tokenized = dataset_squad_train.map(tokenize_and_create_label,\n",
    "                                  batch_size=1000,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=dataset_squad_train.column_names)\n",
    "dataset_squad_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start/end token positions: (97, 99)\n",
      "\n",
      "*** (in tokenized example) ***\n",
      "Answer: the throne room\n",
      "Decoded tokens: [CLS] Which room has The Ballroom replaced in terms of both use and importance? [SEP] Investitures, which include the conferring of knighthoods by dubbing with a sword, and other awards take place in the palace's Ballroom, built in 1854. At 36. 6 m ( 120 ft ) long, 18 m ( 59 ft ) wide and 13. 5 m ( 44 ft ) high, it is the largest room in the palace. It has replaced the throne room in importance and use. During investitures, the Queen stands on the throne dais beneath a giant, domed velvet canopy, known as a shamiana or a baldachin, that was used at the Delhi Durbar in 1911. A military band plays in the musicians'gallery as award recipients approach the Queen and receive their honours, watched by their families and friends. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "*** in original example - index 694***\n",
      "Answer: the throne room\n",
      "Answer_start: 274\n",
      "Question: Which room has The Ballroom replaced in terms of both use and importance?\n",
      "Context: Investitures, which include the conferring of knighthoods by dubbing with a sword, and other awards take place in the palace's Ballroom, built in 1854. At 36.6 m (120 ft) long, 18 m (59 ft) wide and 13.5 m (44 ft) high, it is the largest room in the palace. It has replaced the throne room in importance and use. During investitures, the Queen stands on the throne dais beneath a giant, domed velvet canopy, known as a shamiana or a baldachin, that was used at the Delhi Durbar in 1911. A military band plays in the musicians' gallery as award recipients approach the Queen and receive their honours, watched by their families and friends.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# some testing. This index must be less than 1000 because this is only checking the first batch of the tokenizing process\n",
    "some_idx = 700\n",
    "t = dataset_squad_train_tokenized[some_idx][\"input_ids\"][\n",
    "    dataset_squad_train_tokenized[some_idx][\"start_positions\"] : dataset_squad_train_tokenized[some_idx][\"end_positions\"] + 1\n",
    "    ]\n",
    "\n",
    "print(f\"Start/end token positions: {dataset_squad_train_tokenized[some_idx][\"start_positions\"], dataset_squad_train_tokenized[some_idx][\"end_positions\"]}\\n\")\n",
    "\n",
    "print(\"*** (in tokenized example) ***\")\n",
    "print(f\"Answer: {tokenizer.decode(t)}\")\n",
    "print(f\"Decoded tokens: {tokenizer.decode(dataset_squad_train_tokenized[some_idx][\"input_ids\"])}\")\n",
    "\n",
    "original_example_idx = dataset_squad_train_tokenized[\"overflow_to_sample_mapping\"][some_idx]\n",
    "print(f\"\\n*** in original example - index {original_example_idx}***\")\n",
    "print(f\"Answer: {dataset_squad_train[original_example_idx][\"answers\"][\"text\"][0]}\")\n",
    "print(f\"Answer_start: {dataset_squad_train[original_example_idx][\"answers\"][\"answer_start\"][0]}\")\n",
    "print(f\"Question: {dataset_squad_train[original_example_idx][\"question\"]}\")\n",
    "print(f\"Context: {dataset_squad_train[original_example_idx][\"context\"]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing (Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_squad_valid: Dataset = dataset_squad_mini[\"valid\"]\n",
    "dataset_squad_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing: adding 'example_id' key to map each \"tokenized example\" back to which \"original example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_map_to_original_example_ids(batch):\n",
    "    # print(f\"Using tokenizer checkpoint: {tokenizer.name_or_path}\")\n",
    "    batch_tokenized = tokenizer(text=batch[\"question\"], text_pair=batch[\"context\"],\n",
    "                                                   truncation=\"only_second\", max_length=max_length, \n",
    "                                                   stride=stride,\n",
    "                                                   padding=\"max_length\",\n",
    "                                                   return_overflowing_tokens=True, \n",
    "                                                #    return_length=True,\n",
    "                                                   return_offsets_mapping=True,\n",
    "                                                   )\n",
    "    \n",
    "    # loop through each \"tokenized example\"\n",
    "    # for each \"tokenized example\",\n",
    "    original_example_ids = []\n",
    "    for tokenized_example_idx in range(len(batch_tokenized[\"input_ids\"])):\n",
    "        # find out the corresponding id of the corresponding \"original example\"\n",
    "        id_original_example = batch_tokenized[\"overflow_to_sample_mapping\"][tokenized_example_idx]\n",
    "        original_example_ids.append(batch[\"id\"][id_original_example])\n",
    "    if \"token_type_ids\" not in batch_tokenized.keys():\n",
    "        token_type_ids = []\n",
    "        for tokenized_example_idx in range(len(batch_tokenized[\"input_ids\"])):\n",
    "            token_type_sequence = [1 if token_type == 1 else 0\n",
    "                                   for token_type in batch_tokenized.sequence_ids((tokenized_example_idx))]\n",
    "            token_type_ids.append(token_type_sequence)\n",
    "        batch_tokenized[\"token_type_ids\"] = token_type_ids\n",
    "            \n",
    "    batch_tokenized[\"example_id\"] = original_example_ids\n",
    "    return batch_tokenized\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5312287084440878f1e79ae95861274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'example_id'],\n",
       "    num_rows: 5113\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_squad_valid_tokenized: Dataset = dataset_squad_valid.map(function=tokenize_and_map_to_original_example_ids,\n",
    "                                                        batched=True,\n",
    "                                                        remove_columns=dataset_squad_valid.column_names)\n",
    "dataset_squad_valid_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(eval_batch, model, tokenizer, max_token_distance_to_search=30):\n",
    "    \"\"\"Evaluate each batch, return a dictionary that maps from the `id` of the original examples to the predicted answer text (for this batch)\"\"\"\n",
    "    \n",
    "    # for each batch, generate a look-up table to look up from \"id\" of original example to a list of the multiple \"tokenized examples\" that came from it\n",
    "    from collections import defaultdict\n",
    "    lookup_table = defaultdict(list)\n",
    "    predicted_answers_dict = dict()\n",
    "    for tokenized_example_idx in range(len(eval_batch[\"input_ids\"])):\n",
    "        original_example_id = eval_batch[\"example_id\"][tokenized_example_idx]\n",
    "        lookup_table[original_example_id].append(tokenized_example_idx)\n",
    "        \n",
    "    # pass through the model to predict the answer text positions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_outputs = model(input_ids=eval_batch[\"input_ids\"].to(model.device), \n",
    "                        attention_mask=eval_batch[\"attention_mask\"].to(model.device), \n",
    "                        # token_type_ids=eval_batch[\"token_type_ids\"].to(model.device),\n",
    "                        )\n",
    "    \n",
    "    # loop through each \"original example\"\n",
    "    # and for each \"original example,\" loop through its corresponding \"tokenized examples\"\n",
    "    # meaning: for each \"tokenized example\": get all the start-position logits and end-position logits \n",
    "    # compute the best sum-of-start-and-end-logit score (for all pairs of start-position and end-position logits)\n",
    "    # ignore start-position/end-position pairs where\n",
    "    # (1) start position > end position\n",
    "    # (2) start position or end position is not a the context token\n",
    "    # (3) start position is too far apart from end position\n",
    "    # we do that by computing the sum of logits in a NxN matrix, mask out some unacceptable start-end pairs, then taxke max\n",
    "    \n",
    "    eval_start_logits = eval_outputs[\"start_logits\"]  # for example, torch.Size([32, 100])\n",
    "    eval_end_logits = eval_outputs[\"end_logits\"]  # for example, torch.Size([32, 100])\n",
    "    batch_dim, seq_len = eval_start_logits.shape\n",
    "    eval_start_logits_reshaped = eval_start_logits.reshape(batch_dim, seq_len, 1) # for ex, shape (32, 100, 1)\n",
    "    eval_end_logits_reshaped = eval_end_logits.reshape(batch_dim, 1, seq_len) # for ex, shape (32, 1, 100)\n",
    "    sum_logits = eval_start_logits_reshaped + eval_end_logits_reshaped  # in this matrix A[i, j] (in each batch) is the sum of start-logits at position i and end-logits at position j\n",
    "    \n",
    "    # need to mask out the lower triangular part (below the diagonal) because we only care about start-position < end-position\n",
    "    mask_lower = torch.tril(torch.ones_like(sum_logits), diagonal=-1).to(model.device)  # \"1\" indicates being masked out\n",
    "    \n",
    "    # mask the acceptable start-logits based on which tokens actually belong to the context\n",
    "    mask_start_logits = 1 - eval_batch[\"token_type_ids\"].reshape(batch_dim, seq_len, 1).to(model.device)  # e.g. (32, 100, 1) - \"1\" indicates being masked out\n",
    "    # by inspection, eval_batch[\"token_type_ids\"] has \"0\" values for the questions or padding, and \"1\" values for the tokens belonging to the context\n",
    "    \n",
    "    # mask the acceptable end-logits based on which tokens actually belong to the context\n",
    "    mask_end_logits = 1 - eval_batch[\"token_type_ids\"].reshape(batch_dim, 1, seq_len).to(model.device)  # e.g. (32, 1, 100)\n",
    "    \n",
    "    # mask for only j - i indices < a certain length\n",
    "    i_indices = torch.arange(seq_len).reshape(1, seq_len, 1).expand(batch_dim, seq_len, 1).to(model.device)\n",
    "    j_indices = torch.arange(seq_len).reshape(1, 1, seq_len).expand(batch_dim, 1, seq_len).to(model.device)\n",
    "    distance_mask = (j_indices - i_indices) > max_token_distance_to_search\n",
    "    \n",
    "    final_mask = torch.logical_or(torch.logical_or((mask_lower == 1), (mask_start_logits == 1)), (mask_end_logits == 1))\n",
    "    final_mask = torch.logical_or(final_mask, distance_mask)\n",
    "    \n",
    "    \n",
    "    sum_logits_masked = torch.where(\n",
    "        final_mask,\n",
    "        torch.full_like(sum_logits, float(\"-inf\")),\n",
    "        sum_logits\n",
    "    ).to(model.device)\n",
    "    \n",
    "    max_values, max_indices = torch.max(sum_logits_masked.view(batch_dim, -1), dim=-1)\n",
    "    i_indices = max_indices // sum_logits_masked.shape[1]\n",
    "    j_indices = max_indices % sum_logits_masked.shape[2]\n",
    "    \n",
    "    max_indices_2d = torch.stack((i_indices, j_indices), dim=1)\n",
    "    \n",
    "    # now loop through the original examples (in lookup table)\n",
    "    for original_example_id, tokenized_example_idx_list in lookup_table.items():\n",
    "        # for each \"original example,\" temporarily store the max sum-logit values and the start-end positions (coming from all the \"tokenized examples\" that came from that particular \"original example\")\n",
    "        # then get the largest sum-logit value, and use that particular start/end token positions to extract the answer\n",
    "        temp_list_for_index_pair = []\n",
    "        temp_list_for_max_values = []\n",
    "        for tokenized_example_idx in tokenized_example_idx_list:\n",
    "            temp_list_for_max_values.append(max_values[tokenized_example_idx])\n",
    "            temp_list_for_index_pair.append(max_indices_2d[tokenized_example_idx])\n",
    "        index_of_largest = torch.argmax(torch.tensor(temp_list_for_max_values))\n",
    "        tokenized_example_idx = tokenized_example_idx_list[index_of_largest]\n",
    "        token_position_start = temp_list_for_index_pair[index_of_largest][0].item()\n",
    "        token_position_end = temp_list_for_index_pair[index_of_largest][1].item()\n",
    "        # after this, \"tokenized_example_idx\" is the index of the \"tokenized example\" that has the optimal answer\n",
    "        # \"token_position_start\" is the position of the starting token of the answer (within the \"offset_mapping\" key of the example)\n",
    "        # \"token_position_end\" is the position of the end token of the answer (within the \"offset_mapping\" key of the example)\n",
    "        \n",
    "        # from there, extract the text answer, store in a dict, so we can look up the predicted answer using \"original example id\"\n",
    "        predicted_answer = tokenizer.decode(eval_batch[\"input_ids\"][tokenized_example_idx][token_position_start : token_position_end + 1])\n",
    "        \n",
    "        predicted_answers_dict[original_example_id] = predicted_answer\n",
    "        \n",
    "    return predicted_answers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def compute_metrics(model, valid_loader, dataset_squad_valid, metric):\n",
    "    predicted_answers_entire_valid_set = dict()\n",
    "    # iterate over eval batches\n",
    "    with tqdm.tqdm(desc=\"Eval progress\",\n",
    "               total=len(valid_loader),\n",
    "               unit=\"steps\") as pbar: \n",
    "        for eval_batch_idx, eval_batch in enumerate(valid_loader):\n",
    "            \n",
    "            predicted_answers_dict = evaluate_batch(eval_batch, model, tokenizer)\n",
    "            predicted_answers_entire_valid_set.update(predicted_answers_dict)\n",
    "            pbar.update(1)    \n",
    "    \n",
    "    # then should format the answers as text and format the dictionary as expected by the 'squad' metric\n",
    "    list_predicted_answers = [\n",
    "        {\n",
    "            \"id\": original_example_id,\n",
    "            \"prediction_text\": predicted_answer\n",
    "        }\n",
    "        for original_example_id, predicted_answer in predicted_answers_entire_valid_set.items()\n",
    "    ]\n",
    "        \n",
    "    # now extract the ground-truth answer text from the original dataset, format into expected format for squad\n",
    "    list_ground_truth_answers = []\n",
    "    for original_example_idx in range(len(dataset_squad_valid[\"id\"])):\n",
    "        list_ground_truth_answers.append({\n",
    "            'answers': dataset_squad_valid[\"answers\"][original_example_idx],\n",
    "            'id': dataset_squad_valid[\"id\"][original_example_idx]\n",
    "        })\n",
    "        \n",
    "    eval_results = metric.compute(predictions=list_predicted_answers, references=list_ground_truth_answers)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the eval part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with a DistilBERT model fine-tuned on SQuAD - should get good scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3bfd2ba1ee41768b5db1ba2a816a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52cc533659f4ad1b591bb57049f7ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/HF_NLP/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "def custom_collator(batch):\n",
    "    # Extract the \"example_id\"\n",
    "    example_ids = [ex[\"example_id\"] for ex in batch]\n",
    "    # Use the default data collator to collate\n",
    "    tensor_batch = default_data_collator(batch)\n",
    "    # Add the \"example_id\" back to the collated batch\n",
    "    tensor_batch[\"example_id\"] = example_ids\n",
    "    return tensor_batch\n",
    "\n",
    "valid_loader: DataLoader = DataLoader(dataset=dataset_squad_valid_tokenized,\n",
    "                                      batch_size=32,\n",
    "                                      shuffle=False,\n",
    "                                      collate_fn=custom_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_squad_valid_tokenized: Dataset = dataset_squad_valid.map(function=tokenize_and_map_to_original_example_ids,\n",
    "                                                        batched=True,\n",
    "                                                        remove_columns=dataset_squad_valid.column_names)\n",
    "dataset_squad_valid_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 80.0, 'f1': 86.62500000000001}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"squad\")\n",
    "# info(metric)\n",
    "print(compute_metrics(model, valid_loader, dataset_squad_valid, metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc8520ee9fd4792a28fcce989fc0a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/HF_NLP/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(f\"Using {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_squad_valid_tokenized: Dataset = dataset_squad_valid.map(function=tokenize_and_map_to_original_example_ids,\n",
    "                                                        batched=True,\n",
    "                                                        remove_columns=dataset_squad_valid.column_names)\n",
    "dataset_squad_valid_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_loader: DataLoader = DataLoader(dataset=dataset_squad_train_tokenized,\n",
    "                                      batch_size=32,\n",
    "                                      shuffle=True,\n",
    "                                      collate_fn=default_data_collator)\n",
    "\n",
    "def custom_collator(batch):\n",
    "    # Extract the \"example_id\"\n",
    "    example_ids = [ex[\"example_id\"] for ex in batch]\n",
    "    # Use the default data collator to collate\n",
    "    tensor_batch = default_data_collator(batch)\n",
    "    # Add the \"example_id\" back to the collated batch\n",
    "    tensor_batch[\"example_id\"] = example_ids\n",
    "    return tensor_batch\n",
    "\n",
    "valid_loader: DataLoader = DataLoader(dataset=dataset_squad_valid_tokenized,\n",
    "                                      batch_size=32,\n",
    "                                      shuffle=False,\n",
    "                                      collate_fn=custom_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model (not yet finetuned) on validation set - should get poor score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.0, 'f1': 3.055555555555556}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"squad\")\n",
    "# info(metric)\n",
    "print(compute_metrics(model, valid_loader, dataset_squad_valid, metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: optimizer, lr scheduler, logging/eval params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 2\n",
    "gradient_accumulation_batches = 1\n",
    "logging_steps = 150\n",
    "# eval_steps = 400\n",
    "\n",
    "max_token_distance_to_search = 30\n",
    "\n",
    "# each \"step\" consists of several batches that were accumulated\n",
    "total_training_steps = len(train_loader) // gradient_accumulation_batches * num_epochs\n",
    "\n",
    "lr_scheduler = get_scheduler(name=\"linear\",\n",
    "                             optimizer=optimizer,\n",
    "                             num_warmup_steps=50,\n",
    "                             num_training_steps=total_training_steps,)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(\"Using: \", model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   5%|▍         | 150/3168 [05:26<2:21:04,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 149, 'train loss (last accumulation)': 1.79549241065979} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  10%|▉         | 301/3168 [10:52<1:35:16,  1.99s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 299, 'train loss (last accumulation)': 1.495582103729248} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  14%|█▍        | 450/3168 [16:18<2:06:59,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 449, 'train loss (last accumulation)': 1.1635879278182983} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  16%|█▌        | 501/3168 [18:07<1:48:03,  2.43s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  19%|█▉        | 600/3168 [21:44<2:00:01,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 599, 'train loss (last accumulation)': 2.0094125270843506} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  24%|██▎       | 750/3168 [27:10<1:53:00,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 749, 'train loss (last accumulation)': 1.4823153018951416} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  28%|██▊       | 900/3168 [32:36<1:45:56,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 899, 'train loss (last accumulation)': 1.0121257305145264} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  32%|███▏      | 1001/3168 [36:17<2:03:20,  3.42s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  33%|███▎      | 1050/3168 [38:06<1:38:45,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 1049, 'train loss (last accumulation)': 1.0659204721450806} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  38%|███▊      | 1200/3168 [43:31<1:31:55,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 1199, 'train loss (last accumulation)': 1.1513829231262207} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  43%|████▎     | 1350/3168 [48:57<1:24:59,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 1349, 'train loss (last accumulation)': 1.200542688369751} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  47%|████▋     | 1500/3168 [54:22<1:18:05,  2.81s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 1499, 'train loss (last accumulation)': 0.8191907405853271} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  47%|████▋     | 1501/3168 [54:26<1:28:33,  3.19s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  55%|█████▍    | 1734/3168 [1:02:53<1:07:00,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 1733, 'train loss (last accumulation)': 0.5911881327629089} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  59%|█████▉    | 1884/3168 [1:08:18<1:00:00,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 1883, 'train loss (last accumulation)': 0.9456667900085449} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  63%|██████▎   | 2001/3168 [1:12:34<1:07:05,  3.45s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  64%|██████▍   | 2035/3168 [1:13:48<37:49,  2.00s/steps]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 2033, 'train loss (last accumulation)': 0.2696741223335266} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  69%|██████▉   | 2184/3168 [1:19:13<45:58,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 2183, 'train loss (last accumulation)': 0.6266515851020813} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  74%|███████▎  | 2334/3168 [1:24:39<38:57,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 2333, 'train loss (last accumulation)': 0.6621687412261963} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  78%|███████▊  | 2484/3168 [1:30:04<31:55,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 2483, 'train loss (last accumulation)': 0.6244927644729614} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  79%|███████▉  | 2501/3168 [1:30:43<37:59,  3.42s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  83%|████████▎ | 2634/3168 [1:35:34<24:56,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 2633, 'train loss (last accumulation)': 0.8024786710739136} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  88%|████████▊ | 2784/3168 [1:40:59<17:54,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 2783, 'train loss (last accumulation)': 1.0297491550445557} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  93%|█████████▎| 2934/3168 [1:46:25<10:53,  2.79s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 2933, 'train loss (last accumulation)': 0.5104318857192993} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  95%|█████████▍| 3001/3168 [1:48:52<09:22,  3.37s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  97%|█████████▋| 3084/3168 [1:51:54<03:55,  2.80s/steps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completed_steps': 3083, 'train loss (last accumulation)': 0.8068967461585999} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress: 100%|██████████| 3168/3168 [1:54:54<00:00,  2.18s/steps]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "completed_steps = 0\n",
    "currently_accumulated_losses = []\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "with tqdm.tqdm(desc=\"Training progress\",\n",
    "               total=total_training_steps,\n",
    "               unit=\"steps\") as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # train iteration\n",
    "            # move the inputs to model's device\n",
    "            # pass the inputs through the model\n",
    "            model.train()\n",
    "            outputs = model(input_ids=batch[\"input_ids\"].to(model.device), \n",
    "                  attention_mask=batch[\"attention_mask\"].to(model.device), \n",
    "                #   token_type_ids=batch[\"token_type_ids\"].to(model.device),\n",
    "                  start_positions=batch[\"start_positions\"].to(model.device),\n",
    "                  end_positions=batch[\"end_positions\"].to(model.device))\n",
    "            \n",
    "            # extract the loss\n",
    "            loss_this_batch = outputs.loss\n",
    "            # backward pass\n",
    "            loss_this_batch.backward()\n",
    "            # save the loss value to the currently-accumulated array\n",
    "            currently_accumulated_losses.append(loss_this_batch)\n",
    "            \n",
    "            # if logging step: \n",
    "            if (batch_idx + 1) % (gradient_accumulation_batches * logging_steps) == 0:\n",
    "                accumulated_loss = torch.stack(currently_accumulated_losses).mean().item()\n",
    "                print({\n",
    "                    \"completed_steps\": completed_steps,\n",
    "                    \"train loss (last accumulation)\": accumulated_loss\n",
    "                }, \"\\n\")\n",
    "                \n",
    "            # if accumulation step: update the optimizer, scheduler, \"completed_steps\" value, update progress bar\n",
    "            if (batch_idx + 1) % gradient_accumulation_batches == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step()\n",
    "                completed_steps += 1\n",
    "                pbar.update(1)\n",
    "                currently_accumulated_losses = []\n",
    "            \n",
    "            # if eval step:\n",
    "            # if (batch_idx + 1) % (gradient_accumulation_batches * eval_steps) == 0:\n",
    "                # eval iteration\n",
    "                # print(\"Validation: \", compute_metrics(model, valid_loader, dataset_squad_valid, metric), \"\\n\")\n",
    "                \n",
    "            if completed_steps % 500 == 0:\n",
    "                model.save_pretrained(\"/home/nguyenthuan49/HF_NLP_course/question-answering/fine-tuned-bert-squad\")\n",
    "                print(\"Model saved.\")\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# What a low-tech way to store the training losses and plot - I know...\n",
    "# I forgot to keep track while training  - pls forgive me!\n",
    "\n",
    "# extracted list of tuples with each element containing completed_steps and train_loss\n",
    "# Data extracted\n",
    "data = [\n",
    "    (149, 1.79549241065979),\n",
    "    (299, 1.495582103729248),\n",
    "    (449, 1.1635879278182983),\n",
    "    (599, 2.0094125270843506),\n",
    "    (749, 1.4823153018951416),\n",
    "    (899, 1.0121257305145264),\n",
    "    (1049, 1.0659204721450806),\n",
    "    (1199, 1.1513829231262207),\n",
    "    (1349, 1.200542688369751),\n",
    "    (1499, 0.8191907405853271),\n",
    "    (1733, 0.5911881327629089),\n",
    "    (1883, 0.9456667900085449),\n",
    "    (2033, 0.2696741223335266),\n",
    "    (2183, 0.6266515851020813),\n",
    "    (2333, 0.6621687412261963),\n",
    "    (2483, 0.6244927644729614),\n",
    "    (2633, 0.8024786710739136),\n",
    "    (2783, 1.0297491550445557),\n",
    "    (2933, 0.5104318857192993),\n",
    "    (3083, 0.8068967461585999)\n",
    "]\n",
    "\n",
    "# Unpack data into two lists\n",
    "completed_steps, train_loss = zip(*data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(completed_steps, train_loss, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Completed Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs. Completed Steps')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval progress: 100%|██████████| 160/160 [01:59<00:00,  1.34steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 71.1, 'f1': 80.62985630352878}\n"
     ]
    }
   ],
   "source": [
    "print(compute_metrics(model, valid_loader, dataset_squad_valid, metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/nguyenthuan49/HF_NLP_course/question-answering/fine-tuned-bert-squad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fce5180f69d4b36bc8ce415c4fde820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: How did the military found dead bodies?\n",
      "\n",
      "CONTEXT: The Israeli military said on Saturday that it had found a number of dead bodies during an operation in the Gaza Strip, asking the Israeli public to refrain from speculation about their identities.\n",
      "\n",
      "The announcement was widely interpreted in Israel, however, as confirmation that more Israeli hostages had died in captivity, and it quickly amplified calls for an immediate cease-fire in order to free the roughly 100 captives still held, both dead and alive, in Gaza.\n",
      "\n",
      "ANSWER: during an operation in the Gaza Strip\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "question = \"How did the military found dead bodies?\"\n",
    "context = \"\"\"The Israeli military said on Saturday that it had found a number of dead bodies during an operation in the Gaza Strip, asking the Israeli public to refrain from speculation about their identities.\n",
    "\n",
    "The announcement was widely interpreted in Israel, however, as confirmation that more Israeli hostages had died in captivity, and it quickly amplified calls for an immediate cease-fire in order to free the roughly 100 captives still held, both dead and alive, in Gaza.\"\"\"\n",
    "\n",
    "def find_answer_span(question, context, model, tokenizer):\n",
    "    inference_batch = Dataset.from_dict({\n",
    "        \"id\": [\"00\"],\n",
    "        \"question\": [question],\n",
    "        \"context\": [context]\n",
    "    })\n",
    "\n",
    "    tokenized_inference_batch = inference_batch.map(tokenize_and_map_to_original_example_ids,\n",
    "                                                                    batched=True,\n",
    "                                                                    remove_columns=inference_batch.column_names,\n",
    "                                                                    )\n",
    "    \n",
    "    def custom_collator(batch):\n",
    "        # Extract the \"example_id\"\n",
    "        example_ids = [ex[\"example_id\"] for ex in batch]\n",
    "        # Use the default data collator to collate\n",
    "        tensor_batch = default_data_collator(batch)\n",
    "        # Add the \"example_id\" back to the collated batch\n",
    "        tensor_batch[\"example_id\"] = example_ids\n",
    "        return tensor_batch\n",
    "\n",
    "    inference_loader: DataLoader = DataLoader(dataset=tokenized_inference_batch,\n",
    "                                        batch_size=10,\n",
    "                                        shuffle=False,\n",
    "                                        collate_fn=custom_collator)\n",
    "\n",
    "    # print(compute_metrics(model, train_loader_for_validation, dataset_squad_train, metric))\n",
    "    predicted_answers_dict = evaluate_batch(iter(inference_loader).__next__(), model, tokenizer)\n",
    "    return (predicted_answers_dict['00'])\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2695b45f7e14ed6a13ff5085f1f993e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What was found?\n",
      "\n",
      "CONTEXT: The Israeli military said on Saturday that it had found a number of dead bodies during an operation in the Gaza Strip, asking the Israeli public to refrain from speculation about their identities.\n",
      "\n",
      "The announcement was widely interpreted in Israel, however, as confirmation that more Israeli hostages had died in captivity, and it quickly amplified calls for an immediate cease-fire in order to free the roughly 100 captives still held, both dead and alive, in Gaza.\n",
      "\n",
      "ANSWER: a number of dead bodies\n"
     ]
    }
   ],
   "source": [
    "question = \"What was found?\"\n",
    "context = \"\"\"The Israeli military said on Saturday that it had found a number of dead bodies during an operation in the Gaza Strip, asking the Israeli public to refrain from speculation about their identities.\n",
    "\n",
    "The announcement was widely interpreted in Israel, however, as confirmation that more Israeli hostages had died in captivity, and it quickly amplified calls for an immediate cease-fire in order to free the roughly 100 captives still held, both dead and alive, in Gaza.\"\"\"\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538882567ced4d3ba23146416d4bae2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What was the public not supposed to do?\n",
      "\n",
      "CONTEXT: The Israeli military said on Saturday that it had found a number of dead bodies during an operation in the Gaza Strip, asking the Israeli public to refrain from speculation about their identities.\n",
      "\n",
      "The announcement was widely interpreted in Israel, however, as confirmation that more Israeli hostages had died in captivity, and it quickly amplified calls for an immediate cease-fire in order to free the roughly 100 captives still held, both dead and alive, in Gaza.\n",
      "\n",
      "ANSWER: speculation about their identities\n"
     ]
    }
   ],
   "source": [
    "question = \"What was the public not supposed to do?\"\n",
    "context = \"\"\"The Israeli military said on Saturday that it had found a number of dead bodies during an operation in the Gaza Strip, asking the Israeli public to refrain from speculation about their identities.\n",
    "\n",
    "The announcement was widely interpreted in Israel, however, as confirmation that more Israeli hostages had died in captivity, and it quickly amplified calls for an immediate cease-fire in order to free the roughly 100 captives still held, both dead and alive, in Gaza.\"\"\"\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48e852664eb4b7e85fb79aa28666e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What became more urgent after this announcement?\n",
      "\n",
      "CONTEXT: The Israeli military said on Saturday that it had found a number of dead bodies during an operation in the Gaza Strip, asking the Israeli public to refrain from speculation about their identities.\n",
      "\n",
      "The announcement was widely interpreted in Israel, however, as confirmation that more Israeli hostages had died in captivity, and it quickly amplified calls for an immediate cease-fire in order to free the roughly 100 captives still held, both dead and alive, in Gaza.\n",
      "\n",
      "ANSWER: cease - fire\n"
     ]
    }
   ],
   "source": [
    "question = \"What became more urgent after this announcement?\"\n",
    "context = \"\"\"The Israeli military said on Saturday that it had found a number of dead bodies during an operation in the Gaza Strip, asking the Israeli public to refrain from speculation about their identities.\n",
    "\n",
    "The announcement was widely interpreted in Israel, however, as confirmation that more Israeli hostages had died in captivity, and it quickly amplified calls for an immediate cease-fire in order to free the roughly 100 captives still held, both dead and alive, in Gaza.\"\"\"\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4743218e179b4f4e8d161e2113a06c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What's the situation like for Ukraine at the moment?\n",
      "\n",
      "CONTEXT: President Volodymyr Zelensky of Ukraine on Wednesday was poised to undertake the biggest shake-up of his government since Russia's full-scale invasion, with half a dozen senior figures offering to resign, as Russian forces carried out a second day of deadly attacks in the early morning.\n",
      "\n",
      "Dmytro Kuleba, the foreign minister, has offered to resign as have at least six others, according to the speaker of Ukraine's Parliament. Mr. Zelensky said Tuesday that “personnel decisions have been prepared” to fortify his government in the war. The restructuring comes at a precarious moment for Ukraine in the war, with heavy Russian assaults on the eastern front near the transit hub of Pokrovsk.\n",
      "\n",
      "ANSWER: The restructuring comes at a precarious moment for Ukraine in the war\n"
     ]
    }
   ],
   "source": [
    "question = \"What's the situation like for Ukraine at the moment?\"\n",
    "context = \"\"\"President Volodymyr Zelensky of Ukraine on Wednesday was poised to undertake the biggest shake-up of his government since Russia's full-scale invasion, with half a dozen senior figures offering to resign, as Russian forces carried out a second day of deadly attacks in the early morning.\n",
    "\n",
    "Dmytro Kuleba, the foreign minister, has offered to resign as have at least six others, according to the speaker of Ukraine's Parliament. Mr. Zelensky said Tuesday that “personnel decisions have been prepared” to fortify his government in the war. The restructuring comes at a precarious moment for Ukraine in the war, with heavy Russian assaults on the eastern front near the transit hub of Pokrovsk.\"\"\"\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f360bd33689b4236a29b75a1208f560c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Where's the war raging right now?\n",
      "\n",
      "CONTEXT: President Volodymyr Zelensky of Ukraine on Wednesday was poised to undertake the biggest shake-up of his government since Russia's full-scale invasion, with half a dozen senior figures offering to resign, as Russian forces carried out a second day of deadly attacks in the early morning.\n",
      "\n",
      "Dmytro Kuleba, the foreign minister, has offered to resign as have at least six others, according to the speaker of Ukraine's Parliament. Mr. Zelensky said Tuesday that “personnel decisions have been prepared” to fortify his government in the war. The restructuring comes at a precarious moment for Ukraine in the war, with heavy Russian assaults on the eastern front near the transit hub of Pokrovsk.\n",
      "\n",
      "ANSWER: Pokrovsk\n"
     ]
    }
   ],
   "source": [
    "question = \"Where's the war raging right now?\"\n",
    "context = \"\"\"President Volodymyr Zelensky of Ukraine on Wednesday was poised to undertake the biggest shake-up of his government since Russia's full-scale invasion, with half a dozen senior figures offering to resign, as Russian forces carried out a second day of deadly attacks in the early morning.\n",
    "\n",
    "Dmytro Kuleba, the foreign minister, has offered to resign as have at least six others, according to the speaker of Ukraine's Parliament. Mr. Zelensky said Tuesday that “personnel decisions have been prepared” to fortify his government in the war. The restructuring comes at a precarious moment for Ukraine in the war, with heavy Russian assaults on the eastern front near the transit hub of Pokrovsk.\"\"\"\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662a2d82cc784620a5a0088ddeec35b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What job positions are going to be vacated soon?\n",
      "\n",
      "CONTEXT: President Volodymyr Zelensky of Ukraine on Wednesday was poised to undertake the biggest shake-up of his government since Russia's full-scale invasion, with half a dozen senior figures offering to resign, as Russian forces carried out a second day of deadly attacks in the early morning.\n",
      "\n",
      "Dmytro Kuleba, the foreign minister, has offered to resign as have at least six others, according to the speaker of Ukraine's Parliament. Mr. Zelensky said Tuesday that “personnel decisions have been prepared” to fortify his government in the war. The restructuring comes at a precarious moment for Ukraine in the war, with heavy Russian assaults on the eastern front near the transit hub of Pokrovsk.\n",
      "\n",
      "ANSWER: foreign minister\n"
     ]
    }
   ],
   "source": [
    "question = \"What job positions are going to be vacated soon?\"\n",
    "context = \"\"\"President Volodymyr Zelensky of Ukraine on Wednesday was poised to undertake the biggest shake-up of his government since Russia's full-scale invasion, with half a dozen senior figures offering to resign, as Russian forces carried out a second day of deadly attacks in the early morning.\n",
    "\n",
    "Dmytro Kuleba, the foreign minister, has offered to resign as have at least six others, according to the speaker of Ukraine's Parliament. Mr. Zelensky said Tuesday that “personnel decisions have been prepared” to fortify his government in the war. The restructuring comes at a precarious moment for Ukraine in the war, with heavy Russian assaults on the eastern front near the transit hub of Pokrovsk.\"\"\"\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4eb8eebb2840d9b5c13016180b9869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: How did the US comment on this development?\n",
      "\n",
      "CONTEXT: American and Iraqi commandos raided several Islamic State hide-outs in western Iraq last week, killing at least 14 ISIS fighters in one of the most sweeping counterterrorism missions in the country in recent years.\n",
      "\n",
      "Seven U.S. soldiers were injured as more than 200 troops from both countries, including backup forces, hunted down fighters in bunkers over miles of remote terrain, U.S. and Iraqi officials said, adding that the size, scope and focus of the mission underscored the terrorist organization's resurgence in recent months.\n",
      "\n",
      "A senior insurgent commander overseeing Islamic State operations in the Middle East and Europe was the main target, they said.\n",
      "\n",
      "“The operation targeted ISIS leaders with the objective of disrupting and degrading ISIS's ability to plan, organize, and conduct attacks against Iraqi civilians, as well as U.S., allies and partners throughout the region and beyond,” the military's Central Command said in a statement on Sunday.\n",
      "\n",
      "ANSWER: adding that the size, scope and focus of the mission underscored the terrorist organization's resurgence in recent months\n"
     ]
    }
   ],
   "source": [
    "question = \"How did the US comment on this development?\"\n",
    "context = \"\"\"American and Iraqi commandos raided several Islamic State hide-outs in western Iraq last week, killing at least 14 ISIS fighters in one of the most sweeping counterterrorism missions in the country in recent years.\n",
    "\n",
    "Seven U.S. soldiers were injured as more than 200 troops from both countries, including backup forces, hunted down fighters in bunkers over miles of remote terrain, U.S. and Iraqi officials said, adding that the size, scope and focus of the mission underscored the terrorist organization's resurgence in recent months.\n",
    "\n",
    "A senior insurgent commander overseeing Islamic State operations in the Middle East and Europe was the main target, they said.\n",
    "\n",
    "“The operation targeted ISIS leaders with the objective of disrupting and degrading ISIS's ability to plan, organize, and conduct attacks against Iraqi civilians, as well as U.S., allies and partners throughout the region and beyond,” the military's Central Command said in a statement on Sunday.\"\"\"\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7e74ea59394102af8968b9f67d694c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What was the purpose of this operation?\n",
      "\n",
      "CONTEXT: American and Iraqi commandos raided several Islamic State hide-outs in western Iraq last week, killing at least 14 ISIS fighters in one of the most sweeping counterterrorism missions in the country in recent years.\n",
      "\n",
      "Seven U.S. soldiers were injured as more than 200 troops from both countries, including backup forces, hunted down fighters in bunkers over miles of remote terrain, U.S. and Iraqi officials said, adding that the size, scope and focus of the mission underscored the terrorist organization's resurgence in recent months.\n",
      "\n",
      "A senior insurgent commander overseeing Islamic State operations in the Middle East and Europe was the main target, they said.\n",
      "\n",
      "“The operation targeted ISIS leaders with the objective of disrupting and degrading ISIS's ability to plan, organize, and conduct attacks against Iraqi civilians, as well as U.S., allies and partners throughout the region and beyond,” the military's Central Command said in a statement on Sunday.\n",
      "\n",
      "ANSWER: disrupting and degrading ISIS's ability to plan, organize, and conduct attacks against Iraqi civilians\n"
     ]
    }
   ],
   "source": [
    "question = \"What was the purpose of this operation?\"\n",
    "context = \"\"\"American and Iraqi commandos raided several Islamic State hide-outs in western Iraq last week, killing at least 14 ISIS fighters in one of the most sweeping counterterrorism missions in the country in recent years.\n",
    "\n",
    "Seven U.S. soldiers were injured as more than 200 troops from both countries, including backup forces, hunted down fighters in bunkers over miles of remote terrain, U.S. and Iraqi officials said, adding that the size, scope and focus of the mission underscored the terrorist organization's resurgence in recent months.\n",
    "\n",
    "A senior insurgent commander overseeing Islamic State operations in the Middle East and Europe was the main target, they said.\n",
    "\n",
    "“The operation targeted ISIS leaders with the objective of disrupting and degrading ISIS's ability to plan, organize, and conduct attacks against Iraqi civilians, as well as U.S., allies and partners throughout the region and beyond,” the military's Central Command said in a statement on Sunday.\"\"\"\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fa767fe8364e9a9e823ceacde44aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Where did somme people kill some other people?\n",
      "\n",
      "CONTEXT: American and Iraqi commandos raided several Islamic State hide-outs in western Iraq last week, killing at least 14 ISIS fighters in one of the most sweeping counterterrorism missions in the country in recent years.\n",
      "\n",
      "Seven U.S. soldiers were injured as more than 200 troops from both countries, including backup forces, hunted down fighters in bunkers over miles of remote terrain, U.S. and Iraqi officials said, adding that the size, scope and focus of the mission underscored the terrorist organization's resurgence in recent months.\n",
      "\n",
      "A senior insurgent commander overseeing Islamic State operations in the Middle East and Europe was the main target, they said.\n",
      "\n",
      "“The operation targeted ISIS leaders with the objective of disrupting and degrading ISIS's ability to plan, organize, and conduct attacks against Iraqi civilians, as well as U.S., allies and partners throughout the region and beyond,” the military's Central Command said in a statement on Sunday.\n",
      "\n",
      "ANSWER: bunkers\n"
     ]
    }
   ],
   "source": [
    "question = \"Where did somme people kill some other people?\"\n",
    "context = \"\"\"American and Iraqi commandos raided several Islamic State hide-outs in western Iraq last week, killing at least 14 ISIS fighters in one of the most sweeping counterterrorism missions in the country in recent years.\n",
    "\n",
    "Seven U.S. soldiers were injured as more than 200 troops from both countries, including backup forces, hunted down fighters in bunkers over miles of remote terrain, U.S. and Iraqi officials said, adding that the size, scope and focus of the mission underscored the terrorist organization's resurgence in recent months.\n",
    "\n",
    "A senior insurgent commander overseeing Islamic State operations in the Middle East and Europe was the main target, they said.\n",
    "\n",
    "“The operation targeted ISIS leaders with the objective of disrupting and degrading ISIS's ability to plan, organize, and conduct attacks against Iraqi civilians, as well as U.S., allies and partners throughout the region and beyond,” the military's Central Command said in a statement on Sunday.\"\"\"\n",
    "\n",
    "predicted_answer = find_answer_span(question, context, model, tokenizer)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(f\"\\nCONTEXT: {context}\")\n",
    "print(f\"\\nANSWER: {predicted_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
